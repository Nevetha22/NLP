{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Nevetha NG\n",
        "\n",
        "\n",
        "MDS202128\n"
      ],
      "metadata": {
        "id": "qi7K0G9muJ-6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z09esW0SJ8_N",
        "outputId": "d7c3f43a-22fc-4b02-d005-8554f4162b4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Importing necessary packages\n",
        "import os, re, json, math, nltk, pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV-SHMJ87gYj",
        "outputId": "21f52199-d5fa-44e7-cd73-0b8978e605dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "# Setting paths\n",
        "path_to_json = 'pdf_json/'\n",
        "drive_directory = 'drive/MyDrive/NLP/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdNw11JoKF8Y",
        "outputId": "9a1c6c1b-188e-4a0d-8f99-d6dc684566cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 409 ms, sys: 58.3 ms, total: 468 ms\n",
            "Wall time: 57.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Unzipping json files from drive into colab memory\n",
        "!unzip drive/My\\ Drive/NLP/pdf_json.zip > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8faV3LnKcAW"
      },
      "source": [
        "\n",
        "#### Defining a function to extract the text from the json files and indexing the content using paper_id, title, abstract and body_text fields\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5SsXKBP8SEG"
      },
      "outputs": [],
      "source": [
        "def extract_text(filename):\n",
        "    file = open(filename)\n",
        "    body_text = \"\"\n",
        "    abstract = \"\"\n",
        "    title = \"\"\n",
        "    paper_id = \"\"\n",
        "\n",
        "    paper_content = json.load(file)\n",
        "\n",
        "    #get the paper_id\n",
        "    if 'paper_id' in paper_content:\n",
        "        paper_id = paper_content['paper_id']\n",
        "\n",
        "    #get the title, if available\n",
        "    if 'title' in paper_content['metadata']:\n",
        "        title = paper_content['metadata']['title']\n",
        "\n",
        "    #get abstract.text, if available\n",
        "    if 'abstract' in paper_content:\n",
        "        for abs in paper_content['abstract']:\n",
        "            abstract = abstract + abs['text']\n",
        "    if 'body_text' in paper_content:\n",
        "        for bt in paper_content['body_text']:\n",
        "            body_text = body_text + bt['text']\n",
        "    file.close()\n",
        "\n",
        "    return (title + ' ' + abstract + ' ' + body_text + ' ').lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVfPZOdLK_s2",
        "outputId": "5dbc042a-7f3d-46d6-d21a-3b2e2eb0be4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 17.4 ms, sys: 31 ms, total: 48.4 ms\n",
            "Wall time: 50.6 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Reading all filenames of the json files\n",
        "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpYKHinSPKwO"
      },
      "source": [
        "### 1. Corpus Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GI4tGJeLFrE"
      },
      "outputs": [],
      "source": [
        "def create_corpus(path_to_json, files):\n",
        "  # Creating corpus till the last document\n",
        "  corpus = '\\n'.join(extract_text(path_to_json+i) if re.search('\\n',extract_text(path_to_json+i))\n",
        "                     is None else re.sub('\\n','', extract_text(path_to_json+i))\n",
        "                      for i in files[:50000])\n",
        "\n",
        "  if re.search('\\n', extract_text(path_to_json+files[49999])) is not None:\n",
        "    return corpus + re.sub('\\n','', extract_text(path_to_json+files[49999]))\n",
        "  # Adding the last document to the corpus without \\n at the end\n",
        "  return corpus + extract_text(path_to_json+files[49999])\n",
        "\n",
        "# generating corpus\n",
        "corpus = create_corpus(path_to_json, json_files)\n",
        "\n",
        "# saving corpus\n",
        "with open(drive_directory+'corpus.txt','w') as f:\n",
        "  f.write(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEk2XKECS8j0"
      },
      "source": [
        "### 2 Corpus Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pll49nTLK7E",
        "outputId": "6194da48-8bb2-429b-e4fc-11e9e295eb2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5.66 s, sys: 6.2 s, total: 11.9 s\n",
            "Wall time: 21.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "with open(drive_directory+'corpus.txt','r') as f:\n",
        "  corpus = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UuDl-kYLK9W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e54b39e5-dc77-4d7d-a85c-82aaf9ba8888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ],
      "source": [
        "#Defining few preprocessing functions\n",
        "import nltk\n",
        "nltk.download('words')\n",
        "words = set(nltk.corpus.words.words())\n",
        "words.add(\"integrated\")\n",
        "words.add(\"houses\")\n",
        "def numbers(corpus): # to remove numbers\n",
        "  return re.sub(\"\\d+\", \"\", corpus)\n",
        "def spl_chars(corpus): # to remove special characters\n",
        "  return re.sub(r'[^a-zA-Z0-9.?! ]+', '', corpus)\n",
        "def single_letters(corpus): # to remove single letters except 'i' and 'a'\n",
        "  return \" \".join(w for w in nltk.wordpunct_tokenize(corpus)\n",
        "  if (w =='a' or w=='i' or len(w)>1))\n",
        "def brackets(corpus): # to remove brackets\n",
        "  return re.sub(r\"[\\([{})\\]]\", \"\", corpus)\n",
        "def english_words(corpus): # to remove non-english words\n",
        "  return \" \".join(w for w in nltk.wordpunct_tokenize(corpus) if w.lower() in words)\n",
        "def tokenization(corpus):\n",
        "  return word_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXF-1ly-LK_X"
      },
      "outputs": [],
      "source": [
        "def preprocess_pipeline(corpus):\n",
        "# Defining our sequence of pre-processing steps\n",
        "  corpus = spl_chars(corpus)\n",
        "  corpus = numbers(corpus)\n",
        "  corpus = single_letters(corpus)\n",
        "  corpus = brackets(corpus)\n",
        "  corpus = english_words(corpus)\n",
        "  return corpus\n",
        "\n",
        "def preprocess(corpus, batch_size=500):\n",
        "# Pre-processing the whole corpus in batches to avoid memory-overflow\n",
        "  docs = corpus.split('\\n')\n",
        "  n_batches = math.ceil(len(docs)/batch_size)\n",
        "  for i in range(n_batches):\n",
        "    batch_begin= batch_size*i\n",
        "    batch = docs[batch_begin:batch_begin+batch_size]\n",
        "    #batch = tokenization(batch)\n",
        "    batch = preprocess_pipeline('\\n'.join(batch))\n",
        "    batch = tokenization(batch)\n",
        "    with open(drive_directory+f'token_batch_{i+1}', 'wb') as f:\n",
        "      pickle.dump(batch, f)\n",
        "    print(f'Pre-processing {i+1}-th batch: Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oShGqR6LLBk",
        "outputId": "759b4eb1-5d2b-4352-fafd-a6495abddc1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-processing 1-th batch: Done\n",
            "Pre-processing 2-th batch: Done\n",
            "Pre-processing 3-th batch: Done\n",
            "Pre-processing 4-th batch: Done\n",
            "Pre-processing 5-th batch: Done\n",
            "Pre-processing 6-th batch: Done\n",
            "Pre-processing 7-th batch: Done\n",
            "Pre-processing 8-th batch: Done\n",
            "Pre-processing 9-th batch: Done\n",
            "Pre-processing 10-th batch: Done\n",
            "Pre-processing 11-th batch: Done\n",
            "Pre-processing 12-th batch: Done\n",
            "Pre-processing 13-th batch: Done\n",
            "Pre-processing 14-th batch: Done\n",
            "Pre-processing 15-th batch: Done\n",
            "Pre-processing 16-th batch: Done\n",
            "Pre-processing 17-th batch: Done\n",
            "Pre-processing 18-th batch: Done\n",
            "Pre-processing 19-th batch: Done\n",
            "Pre-processing 20-th batch: Done\n",
            "Pre-processing 21-th batch: Done\n",
            "Pre-processing 22-th batch: Done\n",
            "Pre-processing 23-th batch: Done\n",
            "Pre-processing 24-th batch: Done\n",
            "Pre-processing 25-th batch: Done\n",
            "Pre-processing 26-th batch: Done\n",
            "Pre-processing 27-th batch: Done\n",
            "Pre-processing 28-th batch: Done\n",
            "Pre-processing 29-th batch: Done\n",
            "Pre-processing 30-th batch: Done\n",
            "Pre-processing 31-th batch: Done\n",
            "Pre-processing 32-th batch: Done\n",
            "Pre-processing 33-th batch: Done\n",
            "Pre-processing 34-th batch: Done\n",
            "Pre-processing 35-th batch: Done\n",
            "Pre-processing 36-th batch: Done\n",
            "Pre-processing 37-th batch: Done\n",
            "Pre-processing 38-th batch: Done\n",
            "Pre-processing 39-th batch: Done\n",
            "Pre-processing 40-th batch: Done\n",
            "Pre-processing 41-th batch: Done\n",
            "Pre-processing 42-th batch: Done\n",
            "Pre-processing 43-th batch: Done\n",
            "Pre-processing 44-th batch: Done\n",
            "Pre-processing 45-th batch: Done\n",
            "Pre-processing 46-th batch: Done\n",
            "Pre-processing 47-th batch: Done\n",
            "Pre-processing 48-th batch: Done\n",
            "Pre-processing 49-th batch: Done\n",
            "Pre-processing 50-th batch: Done\n",
            "Pre-processing 51-th batch: Done\n",
            "Pre-processing 52-th batch: Done\n",
            "Pre-processing 53-th batch: Done\n",
            "Pre-processing 54-th batch: Done\n",
            "Pre-processing 55-th batch: Done\n",
            "Pre-processing 56-th batch: Done\n",
            "Pre-processing 57-th batch: Done\n",
            "Pre-processing 58-th batch: Done\n",
            "Pre-processing 59-th batch: Done\n",
            "Pre-processing 60-th batch: Done\n",
            "Pre-processing 61-th batch: Done\n",
            "Pre-processing 62-th batch: Done\n",
            "Pre-processing 63-th batch: Done\n",
            "Pre-processing 64-th batch: Done\n",
            "Pre-processing 65-th batch: Done\n",
            "Pre-processing 66-th batch: Done\n",
            "Pre-processing 67-th batch: Done\n",
            "Pre-processing 68-th batch: Done\n",
            "Pre-processing 69-th batch: Done\n",
            "Pre-processing 70-th batch: Done\n",
            "Pre-processing 71-th batch: Done\n",
            "Pre-processing 72-th batch: Done\n",
            "Pre-processing 73-th batch: Done\n",
            "Pre-processing 74-th batch: Done\n",
            "Pre-processing 75-th batch: Done\n",
            "Pre-processing 76-th batch: Done\n",
            "Pre-processing 77-th batch: Done\n",
            "Pre-processing 78-th batch: Done\n",
            "Pre-processing 79-th batch: Done\n",
            "Pre-processing 80-th batch: Done\n",
            "Pre-processing 81-th batch: Done\n",
            "Pre-processing 82-th batch: Done\n",
            "Pre-processing 83-th batch: Done\n",
            "Pre-processing 84-th batch: Done\n",
            "Pre-processing 85-th batch: Done\n",
            "Pre-processing 86-th batch: Done\n",
            "Pre-processing 87-th batch: Done\n",
            "Pre-processing 88-th batch: Done\n",
            "Pre-processing 89-th batch: Done\n",
            "Pre-processing 90-th batch: Done\n",
            "Pre-processing 91-th batch: Done\n",
            "Pre-processing 92-th batch: Done\n",
            "Pre-processing 93-th batch: Done\n",
            "Pre-processing 94-th batch: Done\n",
            "Pre-processing 95-th batch: Done\n",
            "Pre-processing 96-th batch: Done\n",
            "Pre-processing 97-th batch: Done\n",
            "Pre-processing 98-th batch: Done\n",
            "Pre-processing 99-th batch: Done\n",
            "Pre-processing 100-th batch: Done\n"
          ]
        }
      ],
      "source": [
        "preprocess(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuInk3zqPt5i"
      },
      "source": [
        "### 3. Vocabulary Count"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_files = [batch for batch in os.listdir(drive_directory) if batch.startswith('token_batch')]\n",
        "len(token_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuIMtvYV3vpB",
        "outputId": "a5e331a1-e524-4141-a310-6f89cb58d71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqzEwZyAPxEN"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "vocab = Counter()\n",
        "token_files = [batch for batch in os.listdir(drive_directory) if batch.startswith('token_batch')]\n",
        "for i in token_files:\n",
        "  with open(drive_directory+i, 'r') as f:\n",
        "    vocab.update(pickle.load(f))\n",
        "print(\"The Vocabulary count of the corpus is:\", len(vocab.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_count = 60318"
      ],
      "metadata": {
        "id": "QymkE6LkSZTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnHmIPWeNZs5"
      },
      "source": [
        "### 4. a) Building a Bigram Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCcUDABQPxpX"
      },
      "outputs": [],
      "source": [
        "def createBigram(data):\n",
        "# Creating Bigrams\n",
        "  listOfBigrams = []\n",
        "  bigramCounts = {}\n",
        "  for i in range(len(data)-1):\n",
        "    if i < len(data) -1 and data[i+1]:\n",
        "      listOfBigrams.append((data[i], data[i+1]))\n",
        "      if( data[i], data[i+1]) in bigramCounts:\n",
        "        bigramCounts[(data[i], data[i+1])]+=1\n",
        "      else:\n",
        "          bigramCounts[(data[i], data[i+1])]=1\n",
        "  return bigramCounts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3wrPeuDPxl-"
      },
      "outputs": [],
      "source": [
        "bigrams = Counter()\n",
        "token_files = [batch for batch in os.listdir(drive_directory) if batch.startswith('token_batch')]\n",
        "for i in token_files:\n",
        "  with open(drive_directory+i, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "    bigrams.update(createBigram(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Sa-e21aPxjy"
      },
      "outputs": [],
      "source": [
        "def bigramProbGen(bigrams):\n",
        "# Generating Bigram Probabilities\n",
        "  sorted_bigram_prob = {}\n",
        "  for bigram in bigrams.keys():\n",
        "    b = \" \".join(w for w in (list(bigram)))\n",
        "    p = (bigrams[bigram]+1)/(vocab[bigram[0]]+len(vocab))\n",
        "    sorted_bigram_prob.update({b:p})\n",
        "  return dict(sorted(sorted_bigram_prob.items(), key=lambda x:x[1], reverse = True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_bigram_prob = bigramProbGen(bigrams)"
      ],
      "metadata": {
        "id": "4w6GHM4xLwI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the bigram model"
      ],
      "metadata": {
        "id": "pzcj9ZIn9H5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(drive_directory+\"bigrams.json\", \"wb\") as outfile:\n",
        "    pickle.dump(sorted_bigram_prob, outfile)"
      ],
      "metadata": {
        "id": "6sP_2x907PaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b) Building a Trigram Model"
      ],
      "metadata": {
        "id": "xDiMYxTqhwVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def createTrigram(data):\n",
        "# Creating Trigrams\n",
        "  listOfTrigrams = []\n",
        "  TrigramCounts = {}\n",
        "  for i in range(len(data)-2):\n",
        "    if i < len(data) -2 and data[i+2]:\n",
        "      listOfTrigrams.append((data[i], data[i+1], data[i+2]))\n",
        "      if( data[i], data[i+1], data[i+2]) in TrigramCounts:\n",
        "        TrigramCounts[(data[i], data[i+1], data[i+2])]+=1\n",
        "      else:\n",
        "          TrigramCounts[(data[i], data[i+1], data[i+2])]=1\n",
        "  return TrigramCounts"
      ],
      "metadata": {
        "id": "Q4_r1IQEZAbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#taking only one third of the corpus for trigrams.(due to RAM issues)"
      ],
      "metadata": {
        "id": "tArMao_IZiT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trigrams = Counter()\n",
        "for i in token_files[:33]:\n",
        "  with open(drive_directory+i, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "    trigrams.update(createTrigram(data))"
      ],
      "metadata": {
        "id": "fpHXxYrEW4NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_trigrams = dict(sorted(trigrams.items(), key=lambda x:x[1], reverse = True))"
      ],
      "metadata": {
        "id": "Cmelt3kBe-7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_trigram_prob = {}\n",
        "for trigram in sorted_trigrams.keys():\n",
        "  b = \" \".join(w for w in ([trigram[0],trigram[1]]))\n",
        "  t = \" \".join(w for w in (list(trigram)))\n",
        "  p = (sorted_trigrams[trigram]+1)/(bigram_prob[b]+vocab_count)\n",
        "  sorted_trigram_prob.update({t:p})"
      ],
      "metadata": {
        "id": "9l7pJj56fQx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "saving the trigram model"
      ],
      "metadata": {
        "id": "i82IswAzgsKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(drive_directory+\"trigrams.json\", \"wb\") as outfile:\n",
        "    pickle.dump(sorted_trigram_prob, outfile)"
      ],
      "metadata": {
        "id": "ho7W-Uwoot0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting next word:"
      ],
      "metadata": {
        "id": "qPXw6rzNiGeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Bigram Model"
      ],
      "metadata": {
        "id": "_rL-VnA1h3Nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(drive_directory+\"bigrams.json\", \"rb\") as f:\n",
        "    bigram_prob = pickle.load(f)"
      ],
      "metadata": {
        "id": "QLcvXcjB7bzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_U8CnQ5LPxhA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc5b53fe-1f3c-4000-e8bb-2ce78648ef92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Words for: were\n",
            "{'were in': 0.050263097984826054}\n",
            "{'were with': 0.03875427665411051}\n",
            "{'were by': 0.03095498352307496}\n",
            "{'were and': 0.028638042079531637}\n",
            "{'were to': 0.027819443255924347}\n",
            "{'were not': 0.025694638419894375}\n",
            "{'were used': 0.024905909573085375}\n",
            "{'were from': 0.02116166761658575}\n",
            "{'were the': 0.02052471054377889}\n",
            "{'were for': 0.019438896522327404}\n",
            "Predicted Words for: integrated\n",
            "{'integrated into': 0.022221611444748306}\n",
            "{'integrated with': 0.012436956312614234}\n",
            "{'integrated and': 0.00944109280305633}\n",
            "{'integrated in': 0.007723281157667624}\n",
            "{'integrated approach': 0.004686190168620391}\n",
            "{'integrated the': 0.00393035304464936}\n",
            "{'integrated to': 0.0036005332087347287}\n",
            "{'integrated system': 0.0026523011804801626}\n",
            "{'integrated care': 0.002322481344565531}\n",
            "{'integrated moving': 0.002281253865076202}\n",
            "Predicted Words for: treatment\n",
            "{'treatment of': 0.1504473315699393}\n",
            "{'treatment with': 0.05918391162284114}\n",
            "{'treatment and': 0.05039287381359888}\n",
            "{'treatment for': 0.04152792904932317}\n",
            "{'treatment in': 0.03294305274622686}\n",
            "{'treatment the': 0.023712463046522483}\n",
            "{'treatment is': 0.02047611638400498}\n",
            "{'treatment was': 0.016990819978216897}\n",
            "{'treatment to': 0.01214796950365645}\n",
            "{'treatment or': 0.0092694880970904}\n",
            "Predicted Words for: health\n",
            "{'health and': 0.09236151677950899}\n",
            "{'health care': 0.08199662271083318}\n",
            "{'health organization': 0.032100207808624534}\n",
            "{'health in': 0.028362206282779336}\n",
            "{'health of': 0.022989826634617534}\n",
            "{'health the': 0.01843144390284388}\n",
            "{'health system': 0.018137766583990116}\n",
            "{'health to': 0.015402098515971897}\n",
            "{'health emergency': 0.012197185166741682}\n",
            "{'health status': 0.010010565999189196}\n",
            "Predicted Words for: in\n",
            "{'in the': 0.23540527020540158}\n",
            "{'in a': 0.04563199514918774}\n",
            "{'in this': 0.0322152547939668}\n",
            "{'in and': 0.02773876354816705}\n",
            "{'in of': 0.01753932442333443}\n",
            "{'in with': 0.017529218564462747}\n",
            "{'in addition': 0.016846870973446854}\n",
            "{'in our': 0.01072231626285339}\n",
            "{'in in': 0.010718273919304716}\n",
            "{'in which': 0.010615396275991005}\n"
          ]
        }
      ],
      "source": [
        "words = ['were', 'integrated', 'treatment', 'health', 'in']\n",
        "for word in words:\n",
        "  count = 0\n",
        "  print(\"Predicted Words for:\",word)\n",
        "  for bigrams in bigram_prob.keys():\n",
        "    bigram=bigrams.split(\" \")[0]\n",
        "    if count <10:\n",
        "      if bigram==word:\n",
        "        print({bigrams:bigram_prob[bigrams]})\n",
        "        count+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Trigram Model"
      ],
      "metadata": {
        "id": "BppQb3tW5HIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(drive_directory+\"trigrams.json\", \"rb\") as f:\n",
        "    trigram_prob = pickle.load(f)"
      ],
      "metadata": {
        "id": "cP-_ZwuqXkit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['houses were', 'an integrated', 'and treatment', 'non health', 'work in']\n",
        "for word in words:\n",
        "  count=0\n",
        "  print(\"Predictions for:\",word)\n",
        "  for trigram in trigram_prob.keys():\n",
        "    if count <10:\n",
        "      if trigram.split()[0]==word.split()[0] and trigram.split()[1]==word.split()[1]:\n",
        "        print({trigram:trigram_prob[trigram]})\n",
        "        count+=1"
      ],
      "metadata": {
        "id": "ABcnftUcrTfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "089e4694-ac1d-4c2a-a3c5-28e28fd3eae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions for: houses were\n",
            "{'houses were al': 3.315759772510844e-05}\n",
            "{'houses were temperature': 3.315759772510844e-05}\n",
            "{'houses were for': 3.315759772510844e-05}\n",
            "{'houses were reduced': 3.315759772510844e-05}\n",
            "{'houses were all': 3.315759772510844e-05}\n",
            "{'houses were or': 3.315759772510844e-05}\n",
            "{'houses were the': 3.315759772510844e-05}\n",
            "{'houses were free': 3.315759772510844e-05}\n",
            "{'houses were near': 3.315759772510844e-05}\n",
            "{'houses were not': 3.315759772510844e-05}\n",
            "Predictions for: an integrated\n",
            "{'an integrated approach': 0.0010942006932970353}\n",
            "{'an integrated and': 0.0005636791450318061}\n",
            "{'an integrated system': 0.0003813123628156335}\n",
            "{'an integrated analysis': 0.0003647335644323451}\n",
            "{'an integrated model': 0.0002321031773660378}\n",
            "{'an integrated health': 0.00021552437898274938}\n",
            "{'an integrated platform': 0.00018236678221617255}\n",
            "{'an integrated view': 0.00014920918544959573}\n",
            "{'an integrated strategy': 0.00014920918544959573}\n",
            "{'an integrated one': 0.00014920918544959573}\n",
            "Predictions for: and treatment\n",
            "{'and treatment of': 0.02307768735660796}\n",
            "{'and treatment for': 0.0036639144438292814}\n",
            "{'and treatment with': 0.003614178048664178}\n",
            "{'and treatment in': 0.003166550492178248}\n",
            "{'and treatment the': 0.002602871346973743}\n",
            "{'and treatment and': 0.002586292548585375}\n",
            "{'and treatment are': 0.0015915646452833079}\n",
            "{'and treatment is': 0.0015252494517298367}\n",
            "{'and treatment were': 0.0015086706533414687}\n",
            "{'and treatment to': 0.0012268310807392164}\n",
            "Predictions for: non health\n",
            "Predictions for: work in\n",
            "{'work in the': 0.006233624185864716}\n",
            "{'work in this': 0.005073109044879264}\n",
            "{'work in a': 0.002188399980143996}\n",
            "{'work in that': 0.0013594605937258157}\n",
            "{'work in and': 0.0009781484759734528}\n",
            "{'work in our': 0.0006465727214061807}\n",
            "{'work in an': 0.0005139424195792718}\n",
            "{'work in progress': 0.0004973636318509082}\n",
            "{'work in addition': 0.00034815454229563574}\n",
            "{'work in these': 0.00034815454229563574}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6 Perplexity Score"
      ],
      "metadata": {
        "id": "lbKSecr5JXkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Bigram Model"
      ],
      "metadata": {
        "id": "aVxm1DQLoATx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity_scoreb(sentence):\n",
        "  N = len(sentence.split())\n",
        "  bigrams = list(createBigram(word_tokenize(sentence)).keys())\n",
        "  perplexity = 1\n",
        "  for bigram in bigrams:\n",
        "    key = \" \".join(w for w in (list(bigram)))\n",
        "    try:\n",
        "      if bigram_prob[key]:\n",
        "        p = bigram_prob[key]\n",
        "        perplexity = perplexity*(1/p)\n",
        "    except:\n",
        "      pass\n",
        "  perplexity = pow(perplexity,1/N)\n",
        "  return perplexity"
      ],
      "metadata": {
        "id": "K96apJIbJWqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = 'that the overall code stroke volume has decreased since the covid pandemic'\n",
        "perplexity_scoreb(sentence1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSy3oL7WudGH",
        "outputId": "1e489d56-9e79-4c11-e167-dcf37dafa1ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29.66642255856188"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence2='half a century ago hypertension was not treatable'\n",
        "perplexity_scoreb(sentence2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Avexsa9H0dU-",
        "outputId": "fa3115d7-3a43-4a7f-bf7e-b82232b45f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "408.35495849352606"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence3='sarahs tv is broadcast an advert for private healthcare'\n",
        "perplexity_scoreb(sentence3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab9E0ody066g",
        "outputId": "ace939c3-e476-4040-b5f7-aaf2752c86ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "94.78888092781801"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Trigram Model"
      ],
      "metadata": {
        "id": "NHgbquqeoEiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity_scoret(sentence):\n",
        "  N = len(sentence.split())\n",
        "  trigrams = list(createTrigram(word_tokenize(sentence)).keys())\n",
        "  perplexity = 1\n",
        "  for trigram in trigrams:\n",
        "    key = \" \".join(w for w in (list(trigram)))\n",
        "    try:\n",
        "      if trigram_prob[key]:\n",
        "        p = trigram_prob[key]\n",
        "        perplexity = perplexity*(1/p)\n",
        "    except:\n",
        "      pass\n",
        "  perplexity = pow(perplexity,1/N)\n",
        "  return perplexity"
      ],
      "metadata": {
        "id": "XmKshUs6fDnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = 'that the overall code stroke volume has decreased since the covid pandemic'\n",
        "perplexity_scoret(sentence1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f22d2a7-0eef-4d73-a56b-e64e4f242f09",
        "id": "bLcbuatYnEBe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.227458410316034"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence2='half a century ago hypertension was not treatable'\n",
        "perplexity_scoret(sentence2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55121abc-2ba7-41a2-b57d-038b02972a07",
        "id": "pcQ1Zd5snEBk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63.514661887080486"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence3='sarahs tv is broadcast an advert for private healthcare'\n",
        "perplexity_scoret(sentence3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "517a0d9e-396c-4e18-a31a-03b62d3246f5",
        "id": "IJXPukyhnEBk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the perplexity score has improved for the Trigram model"
      ],
      "metadata": {
        "id": "4vvQNjQ0vPkl"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}